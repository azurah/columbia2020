{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Statistics\n",
    "\n",
    "So far, we have looked at an approach to inference that is grounded in one view of probability -- the classical or frequentist view. As Larry Wasserman at CMU puts it \"In frequentist inference, probabilities are interpreted as long run frequencies. The goal is to create procedures with long run frequency guarantees.\" And we have seen how the idea of long run frequencies of repeated trials is behind constructions like P-values and confidence intervals. \n",
    "\n",
    "By contrast, \"In Bayesian inference, probabilities are interpreted as subjective degrees of belief. The goal is to state and analyze your beliefs.\" Through the formalism of a Bayesian analysis we take our prior belief about some phenomenon, expressed through a probability distribution, and \"update\" it after seeing data. Now, to make this idea real, we need to revist the some basic probability. \n",
    "\n",
    "**Conditional probability**\n",
    "\n",
    "We'll start with something we've seen before — a two-by-two table, our friend from randomized controlled trials. There are four possible outcomes. \n",
    "\n",
    "<img src=\"https://github.com/computationaljournalism/columbia2020/raw/master/stats-breakfasts/tmp.001.jpeg\" width=300>\n",
    "\n",
    "We can also express the different cells in the table using a Venn diagram — named after John Venn who, along with developing this kind of graphic representation, also struggled with the \"repeated trials\" interpretation of probability. Let $A$ be event that someone has had COVID-19, and let $B$ represent the event that they have a positive result on a particular test for antibodies to the virus. (OK that's probably trying too hard as an example). \n",
    "\n",
    "<img src=\"https://github.com/computationaljournalism/columbia2020/raw/master/stats-breakfasts/tmp.002.jpeg\" width=400>\n",
    "\n",
    "Suppose we want to calculate the probability of event $A$ happening given that $B$ has  occurred. The portion of $A$ that is relevant to us now is just the intersection of  $A$ and $B$, denoted $A\\cap B$. The *conditional probability* of $A$ given $B$ is\n",
    "\n",
    "$$P(A|B) = \\frac{P(A\\cap B)}{P(B)}$$\n",
    "\n",
    "So knowing that $B$ has occured, we look for the relative size of $A$ in $B$. This can work the other way as well, writing \n",
    "\n",
    "$$P(B|A) = \\frac{P(A\\cap B)}{P(A)}$$\n",
    "\n",
    "Now, we say $A$ and $B$ are *independent* if their probabilities multiply — that is, if $P(A\\cap B) = P(A)P(B)$, or put another way, $P(A|B) = P(A)$.\n",
    "\n",
    "**Bayes' Rule**\n",
    "\n",
    "There is a person behind the term \"Bayesian\" in Bayesian Statistics — an English minister, Thomas Bayes. If we combine the two equations\n",
    "\n",
    "$$P(A|B) = \\frac{P(A\\cap B)}{P(B)}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$P(B|A) = \\frac{P(A\\cap B)}{P(A)}$$\n",
    "\n",
    "we can see that\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "This last expression is called Bayes' Theorem and was presented to the Royal Society after his death in 1761. We can use the table above to show how it works. \n",
    "\n",
    "Let $A$ be the event that someone has had COVID-19 and $B$ be the event that they test positive using a particular antibody test. The *sensitivity* of the test is defined to be $P(B|A)$ — the conditional probability that someone tests positive given that they have had the virus. \n",
    "\n",
    "The *specificity* of the test is defined to be $P(B^c|A^c)$, where the \"c\" means the *complement* of the set. In other words, it is the conditional probability that someone tests negative given they haven't had the virus. \n",
    "\n",
    "Suppose a test for COVID-19 antibodies has a sensitivity of 90% and a specificity of 80%. Assume also that 20% of the population has had COVID-19. What is the chance that someone who tests positive for the antibodies really had the virus?\n",
    "\n",
    "Well, $P(B|A)$ is 0.9, and the prevalence of COVID or $P(A)$ is 0.3. To use Bayes' Theorem, we just need to compute $P(B)$, the chance that someone receives a positive antibody test. But recalling our axioms of probability, we know that the probabilities of disjoint events add, so\n",
    "\n",
    "$$P(B) = P(B\\cap A) + P(B \\cap A^c)$$\n",
    "\n",
    "This is basically summing the entries in the top row of our two-by-two table above. We also know that $P(B\\cap A) = P(B|A)P(A)$ or the specificity of the test times the prevalence. The second term is $P(B\\cap A^c) = P(B|A^c)P(A^c)$. So we can write\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)} = \\frac{P(B|A)P(A)}{P(B|A)P(A)+ P(B|A^c)P(A^c)}$$\n",
    "\n",
    "Pretty, right? Now to compute the quantity we're after, the chance that someone really had the virus given that they test positive.\n",
    "\n",
    "Recalling again our axioms of probability, $P(A^c) = 1-P(A)$. Since $P(A)$ is the prevalence of COVID-19, this is 1-0.3 = 0.7. Similarly, the conditional probability $P(B|A^c) = 1-P(B^c|A^c)$. We recognize the second term as the specificity of the test, so this is 1-0.8 = 0.2.\n",
    "\n",
    "So, putting it all together\n",
    "\n",
    "$$P(A|B) = \\frac{0.9\\times 0.3}{0.9\\times 0.3+ 0.2\\times 0.7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9*0.3/(0.9*0.3+0.2*0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayesian inference**\n",
    "\n",
    "Bayes' Rule gives us a formal way to update our beliefs. Let's see how this works in a simple case. Suppose $A$ is the event that a coin is fair (it always comes back to games of chance) and let $A^c$ be that both sides of the coin has a \"head\" (let's ignore all the other ways a coin might be unfair). \n",
    "\n",
    "We will let $P(A)$ denote our *prior* belief that a coin is fair. Suppose we collect some data, tossing the coin once and we get heads or $H$. The chance of seeing $H$ given the coin is fair is 0.5. That is, $P(H|A) = 0.5$. The chance that we see $H$ if the coin is double-headed is 1. Or, in symbols $P(H|A^c)$ = 1. Now, having seen one toss of a coin come up $H$, we have from Bayes' Theorem\n",
    "\n",
    "$$P(A|H) = \\frac{P(H|A)P(A)}{P(H|A)P(A)+P(H|A^c)P(A^c)}$$\n",
    "\n",
    "Plugging in the numbers\n",
    "\n",
    "$$P(A|H) = \\frac{0.5\\times 0.9}{0.5\\times 0.9 + 1\\times 0.1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5*0.9/(0.5*0.9+1*0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way we have \"updated\" our belief that the coin is fair after one toss. We have moved from our prior belief $P(A) = 0.9$ to our *posterior* belief $P(A|H) = 0.82$. How about a second toss? Suppose that one also comes up heads. So now we have $HH$ or two heads.\n",
    "\n",
    "$$P(A|HH) = \\frac{P(HH|A)P(A)}{P(HH|A)P(A)+P(HH|A^c)P(A^c)}$$\n",
    "\n",
    "The chance of seeing two heads in a row given the coin is fair, $P(HH|A)$, is just $0.5\\times 0.5 = 0.25$. And the chance of seeing two heads in a row given the coin is double headed is, well, 1. So we get...\n",
    "\n",
    "$$P(A|HH) = \\frac{0.25\\times 0.9}{0.25\\times 0.9 + 1\\times 0.1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.25*0.9/(0.25*0.9+1*0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now moved from a prior belief that the coin is fair of 0.9 to a posterior value of 0.7 given two tosses having turned up heads. And after 3 heads? $HHH$?\n",
    "\n",
    "$$P(A|HHH) = \\frac{P(HHH|A)P(A)}{P(HHH|A)P(A)+P(HHH|A^c)P(A^c)} = \\frac{0.125\\times 0.9}{0.125\\times 0.9 + 1\\times 0.1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.125*0.9/(0.125*0.9+1*0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after 4 heads? $HHH$?\n",
    "\n",
    "$$P(A|HHHH) = \\frac{P(HHHH|A)P(A)}{P(HHHH|A)P(A)+P(HHHH|A^c)P(A^c)} = \\frac{0.0625\\times 0.9}{0.0625\\times 0.9 + 1\\times 0.1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.0625*0.9/(0.0625*0.9+1*0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are, with experience, updating our belief and coming around to the fact that this coin is not fair. Now, had we been more skeptical to begin with and set our prior to be $P(A) = 0.8$, we would we wondering about the coin sooner. In that case, $P(A|HH)$ would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.25*0.8/(0.25*0.8+1*0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the same token, a stronger belief that the coin is fair, say $P(A) = 0.99$, would require more evidence to move us. Here $P(A|HHHH)$ would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.0625*0.99/(0.0625*0.99+1*0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, but even after thousands of $H$'s (remember those poor mathematicians and prisoners of war tossing coins thousands of times), even one \"tail\" (or $T$) is enough to restore our belief that the coin is fair. The second term in the denominator involves\n",
    "\n",
    "$$P(HHHHH...T|A^c) = 0$$\n",
    "\n",
    "since a double-headed coin could never toss a $T$. That means our posterior belief is\n",
    "\n",
    "$$P(A|HHHH...T) = 1$$\n",
    "\n",
    "And this is where we see that this is just a toy model. A coin that's fair tossing $H$ thousands of times with just one $T$ is certainly not fair, but we assumed at the beginning that coins were either fair or two-headed. \n",
    "\n",
    "We finish with a little vocabulary. $P(A)$ is our *prior belief* in a hypothesis or model (a coin is fair). $P(H|A)$ is the *likelihood* of observations given the hypothesis or model is true. And $P(A|H)$ is our *posterior belief* in the hypothesis given data.\n",
    "\n",
    "**Another example, also involving coins**\n",
    "\n",
    "A more realistic model for coin tossing would involve estimating the chance $p$ that a coin will toss heads. Suppose we express our prior beliefs about $p$ through a probability function. For coin tossing, the natural choice is something called the \"beta\" distribution. It has two *parameters* that we can use to express our beliefs about how coins behave. Here is the math for the beta \n",
    "\n",
    "$$\\pi(p) \\propto p^{\\alpha-1}(1-p)^{\\beta-1}$$\n",
    "\n",
    "The symbol $\\propto$ just means that there's a constant (just some number like 5 or 12 or 13.4) that we can compute so that the area under the curve is 1. Let's draw some of these curves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=6, repr.plot.height=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "\n",
    "# let's try two values of alpha and beta to get things rolling (or flipping)\n",
    "alpha = 5\n",
    "beta = 5\n",
    "\n",
    "p = seq(0,1,len=200)\n",
    "prior = dbeta(p,alpha,beta)\n",
    "\n",
    "ggplot()+geom_line(aes(x=p,y=prior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower alpha\n",
    "alpha = 2\n",
    "beta = 5\n",
    "\n",
    "p = seq(0,1,len=200)\n",
    "prior = dbeta(p,alpha,beta)\n",
    "\n",
    "ggplot()+geom_line(aes(x=p,y=prior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower beta\n",
    "alpha = 5\n",
    "beta = 2\n",
    "\n",
    "p = seq(0,1,len=200)\n",
    "prior = dbeta(p,alpha,beta)\n",
    "\n",
    "ggplot()+geom_line(aes(x=p,y=prior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase alpha and beta\n",
    "alpha = 15\n",
    "beta = 15\n",
    "\n",
    "p = seq(0,1,len=200)\n",
    "prior = dbeta(p,alpha,beta)\n",
    "\n",
    "ggplot()+geom_line(aes(x=p,y=prior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase alpha and beta\n",
    "alpha = 50\n",
    "beta = 50\n",
    "\n",
    "p = seq(0,1,len=200)\n",
    "prior = dbeta(p,alpha,beta)\n",
    "\n",
    "ggplot()+geom_line(aes(x=p,y=prior))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that as an expression of our beliefs, the beta has a great deal of flexibility. It can place more weight near 0 (meaning the chance of seeing heads is less) or near 1 (increasing the chance of seeing heads) or right at a half (the coin is fair). The mean of the beta distribution is $\\alpha/(\\alpha+\\beta)$.\n",
    "\n",
    "We can control the \"strength\" of our beliefs by making $\\alpha$ and $\\beta$ larger. In the plot above, we are pretty tightly arranged at 0.5 and it will take a lot of data to convince us that our coin is not fair. By contrast $\\alpha=\\beta=1$ is a \"uniform\" distribution, meaning we have preferences about our coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase alpha and beta\n",
    "alpha = 1\n",
    "beta = 1\n",
    "\n",
    "p = seq(0,1,len=200)\n",
    "prior = dbeta(p,alpha,beta)\n",
    "\n",
    "ggplot()+geom_line(aes(x=p,y=prior))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose we toss a coin $N$ times and $n$ of the tosses are $H$. We can do a little math and find out that the posterior distribution for $p$ is again a beta and its new \"updated\" parmeters are $\\alpha +n$ and $\\beta+N-n$. Or, in words, $\\alpha+(\\mbox{number of heads})$ and $\\beta+(\\mbox{number of tails})$\n",
    "\n",
    "Let's try this out. The green lines are our priors, the blue lines our posterior beliefs. First, the uniform prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ten tosses, two heads\n",
    "N = 10\n",
    "n = 2\n",
    "\n",
    "alpha = 1\n",
    "beta = 1\n",
    "\n",
    "m = alpha/(alpha+beta)\n",
    "mp = (alpha+n)/(alpha+beta+N)\n",
    "\n",
    "p = seq(0,1,len=200)\n",
    "prior = dbeta(p,alpha,beta)\n",
    "posterior = dbeta(p,alpha+n,beta+N-n)\n",
    "\n",
    "ggplot()+geom_line(aes(x=p,y=prior),color=\"green\")+\n",
    "    geom_line(aes(x=p,y=posterior),color=\"blue\")+\n",
    "    geom_vline(xintercept=m,color=\"green\",linetype=\"dotted\")+\n",
    "    geom_vline(xintercept=mp,color=\"blue\",linetype=\"dotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose instead we had some fairly strong beliefs that the coin is fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ten tosses, two heads\n",
    "N = 10\n",
    "n = 2\n",
    "\n",
    "alpha = 10\n",
    "beta = 10\n",
    "\n",
    "m = alpha/(alpha+beta)\n",
    "mp = (alpha+n)/(alpha+beta+N)\n",
    "\n",
    "p = seq(0,1,len=200)\n",
    "prior = dbeta(p,alpha,beta)\n",
    "posterior = dbeta(p,alpha+n,beta+N-n)\n",
    "\n",
    "ggplot()+geom_line(aes(x=p,y=prior),color=\"green\")+\n",
    "    geom_line(aes(x=p,y=posterior),color=\"blue\")+\n",
    "    geom_vline(xintercept=m,color=\"green\",linetype=\"dotted\")+\n",
    "    geom_vline(xintercept=mp,color=\"blue\",linetype=\"dotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more concentrated our prior beliefs, the slower we are to accept signals from the data. But eventually, the data will overcome our prior beliefs. Let's toss 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 tosses, two heads\n",
    "N = 100\n",
    "n = 2\n",
    "\n",
    "alpha = 1\n",
    "beta = 1\n",
    "\n",
    "m = alpha/(alpha+beta)\n",
    "mp = (alpha+n)/(alpha+beta+N)\n",
    "\n",
    "p = seq(0,1,len=300)\n",
    "prior = dbeta(p,alpha,beta)\n",
    "posterior = dbeta(p,alpha+n,beta+N-n)\n",
    "\n",
    "ggplot()+geom_line(aes(x=p,y=prior),color=\"green\")+\n",
    "    geom_line(aes(x=p,y=posterior),color=\"blue\")+\n",
    "    geom_vline(xintercept=m,color=\"green\",linetype=\"dotted\")+\n",
    "    geom_vline(xintercept=mp,color=\"blue\",linetype=\"dotted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 tosses, two heads\n",
    "N = 100\n",
    "n = 2\n",
    "\n",
    "alpha = 10\n",
    "beta = 10\n",
    "\n",
    "m = alpha/(alpha+beta)\n",
    "mp = (alpha+n)/(alpha+beta+N)\n",
    "\n",
    "p = seq(0,1,len=300)\n",
    "prior = dbeta(p,alpha,beta)\n",
    "posterior = dbeta(p,alpha+n,beta+N-n)\n",
    "\n",
    "ggplot()+geom_line(aes(x=p,y=prior),color=\"green\")+\n",
    "    geom_line(aes(x=p,y=posterior),color=\"blue\")+\n",
    "    geom_vline(xintercept=m,color=\"green\",linetype=\"dotted\")+\n",
    "    geom_vline(xintercept=mp,color=\"blue\",linetype=\"dotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of data will even swamp out a bad set of strongly held beliefs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 tosses, two heads\n",
    "N = 100\n",
    "n = 2\n",
    "\n",
    "alpha = 10\n",
    "beta = 2\n",
    "\n",
    "m = alpha/(alpha+beta)\n",
    "mp = (alpha+n)/(alpha+beta+N)\n",
    "\n",
    "p = seq(0,1,len=300)\n",
    "prior = dbeta(p,alpha,beta)\n",
    "posterior = dbeta(p,alpha+n,beta+N-n)\n",
    "\n",
    "ggplot()+geom_line(aes(x=p,y=prior),color=\"green\")+\n",
    "    geom_line(aes(x=p,y=posterior),color=\"blue\")+\n",
    "    geom_vline(xintercept=m,color=\"green\",linetype=\"dotted\")+\n",
    "    geom_vline(xintercept=mp,color=\"blue\",linetype=\"dotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the posterior mean is $(\\alpha+n)/(\\alpha+\\beta+N)$. So as $N$ (and $n$) get big, they eventually overcome $\\alpha$ and $\\beta$ and our posterior distribution centers on $n/N$. Look familiar? It's just the proportion of heads, which is what we would have guessed if we were a frequentist!\n",
    "\n",
    "This expression also gives us a way to calibrate our choice of $\\alpha$ and $\\beta$. They are like a prior number of successes and failures (Heads and Tails) that you've seen. If you've tossed a coin a lot of times, they might be large, if your experience is meager, you might set them small and widen the prior distribution.\n",
    "\n",
    "Whereas frequentists based their inference on constructions like confidence intervals, they couldn't say anything directly about the sample in hand. They just know if they repeated the trials a number of times, they can control the errors they'd make.\n",
    "\n",
    "The Bayesians can create intervals that describe where their updated beliefs concentrate. Their probability statements are about the given sample, making them much more natural. We can use the upper an lower quantiles of the posterior beta distribution to get an interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 tosses, five heads\n",
    "N = 20\n",
    "n = 5\n",
    "\n",
    "alpha = 1\n",
    "beta = 1\n",
    "\n",
    "alphap = alpha+n\n",
    "betap = beta+N-n\n",
    "mp = (alphap)/(alphap+betap)\n",
    "\n",
    "lower = qbeta(0.025,alphap,betap)\n",
    "upper = qbeta(0.975,alphap,betap)\n",
    "\n",
    "round(c(lower,mp,upper),digits=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with more data, our intervals get narrower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200 tosses, 50 heads\n",
    "N = 200\n",
    "n = 50\n",
    "\n",
    "alpha = 1\n",
    "beta = 1\n",
    "\n",
    "alphap = alpha+n\n",
    "betap = beta+N-n\n",
    "mp = (alphap)/(alphap+betap)\n",
    "\n",
    "lower = qbeta(0.025,alphap,betap)\n",
    "upper = qbeta(0.975,alphap,betap)\n",
    "\n",
    "round(c(lower,mp,upper),digits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/sqrt(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian approach to learning has a lot to recommend it. It provides a formal technique for updating our prior beliefs given new observations. We can create analogs of confidence intervals and perform hypothesis testing. While frequentists view a parameter of interest as fixed but unknown, the Bayesian thinks in terms of beliefs. \n",
    "\n",
    "Bayesian approaches are sometimes treated skeptically because setting the prior can be difficult. How do you express your beliefs about a coin or about some phenomenon that you are unfamiliar with? There have been decades of work on how to pick a prior. A dear friend and advisor of mine summarized the dilema this way - “ ... it is easy to prove that for essentially any pair of Bayesians, each thinks the other is crazy.”\n",
    "\n",
    "For many years, there has been a debate about which style of reasoning is to be preferred. I think that debate has largely softened as people have opened up to probability modeling more generally. Larry Wasserman puts it this way.\n",
    "\n",
    ">What should we conclude from all this? The important thing is to understand that frequentist and Bayesian methods are answering different questions. Much of the debate about\n",
    "Bayesian and frequentist inference stems from the fact that people confuse the two. We\n",
    "can summarize this as follow:\n",
    "<br><br>\n",
    "To analyze subjective beliefs in a principled way: use Bayesian inference.\n",
    "<br><br>\n",
    "To design methods with long run frequence guarantees: ue frequentist inference.\n",
    "<br><br>\n",
    "In general, Bayesian methods does not have good freqency performance and freqentist\n",
    "methods to do represent anyone’s subjective beliefs. They are different tools and there is\n",
    "no reason they should be the same. As it happenns, in low-dimensional models with lots\n",
    "of data, they do tend to be similar. But in high-dimensional models they are quite different.\n",
    "Generally, Bayesian methods have poor frequentist behavior when the parameter space is\n",
    "high dimensional."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
